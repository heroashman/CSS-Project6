---
title: "R Notebook"
output:
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---


```{r, include = FALSE}
rm(list = ls())

# set up 
library(pacman)
p_load(tidyverse, data.table, here, MatchIt, cobalt, ggplot2, parallel)

if(Sys.getenv("USER") == "alexramiller"){
  hawd <- "~/Git/CSS-Project6-Team2"
} else {
  hawd <- "C:/Users/heroa/Google Drive/BerkeleyCourses/SOC273L/CSS-Project6-Team2/"
}
setwd(hawd)

d <- fread("data/ypsps.csv")
head(d)
colnames(d)

# SET TO TRUE IF RE-RUNNING SIMILATION OR FALSE IS NOT
RUNSIM <- FALSE

set.seed(5)
```

## Question 3

```{r random assignment}
# Generate a vector that randomly assigns each unit to treatment/control
# Treatment vector is called A

d <- d %>% mutate(A = 
          recode(as.numeric(rbernoulli(dim(.)[1], p = 0.5)), '0' = "Control", '1' = "Treatment"))

# plot histogram of baseline covariate of whether parent owned their home
# Visualize the distribution by treatment/control (ggplot)

ggplot(d, aes(x=parent_OwnHome))+
  geom_histogram(color="black", fill="white", bins = 2)+
  facet_grid(A ~ .)
```

```{r randomization simulation}
# Simulate first two steps 10,000 times (monte carlo simulation - see R Refresher for a hint)
iters <- 10000

n <- iters * dim(d)[1]

# Create a matrix of random assignments, each column is an iteration
set.seed(5)
randomizations <- matrix(rep(as.numeric(rbernoulli(n, p = 0.5))), 
                         nrow = dim(d)[1], ncol = iters)

# Drop the initial assignment
d <- d %>% select(-A)

# Initialize results vector (don't know if this is necessary since I use apply?)
results <- rep(0, 10000)

# Calculate ratio of control to treatment for each randomization iteration and store in results
results <- apply(randomizations, 2, function(x) 
  list(d %>% filter(x == 1) %>% select(parent_OwnHome) %>% sum() /
    d %>% filter(x == 0) %>% select(parent_OwnHome) %>% sum(), 
    
    d %>% filter(x == 1) %>% select(parent_OwnHome) %>% -1 %>% sum() %>% abs() /
    d %>% filter(x == 0) %>% select(parent_OwnHome) %>% -1 %>% sum() %>% abs()
    
    ))

# Turn into dataframe and give column names
results <- data.frame(t(matrix(unlist(results), nrow = 2, ncol = 10000)))
colnames(results) <- c("Covariate == 1", "Covariate == 0")


# Plotting both densities 
p <- ggplot(reshape2::melt(results),aes(x=value, fill=variable)) + geom_density(alpha = 0.25)
  + xlab("Ratio of treatment to control on homeownership") 

p + ggtitle("Density of 10k simulated ratios") + labs()


  
```

### 3.1: Questions 

**1. **
For both values of the binary covariate (whether the parent owned their home or not), the ratio of the units between the randomized treatment and control seemed to center around 1. This indicates that on average, independence of the treatment assignment and baseline covariates does result in covariate balance across exposures. However, because assignment is a process, covariate balance is not always guaranteed despite its independence. It is possible to draw a sample, for instance, that is in the tails and outside of the 0.1 balance threshold. Furthermore, as exemplified by the covariate that we chose for this simulation, imbalance in the covariate itself also affects the likelihood of a balanced draw. Because many more units had a value of 1 rather than 0, the balance on the latter is more spread out in the simulations than the former. Thus, matching could still be useful even in completely randomized studies. 


## Question 4

### 4.1: One Model
The goal of propensity score matching is to achieve a balance between the treatment and control groups in variables that may explain selection into treatment. In the Kam and Palmer (2008) paper, they use more than 80 covariates to predict assignment to treatment (attending college). Henderson and Chatfield (2009) demonstrate that after matching students based on these 80+ covariates the balance between treatment and control groups actually gets worse (though improves on several key variables). Both sets of authors agree that there are a number of attributes that explain why some people go to college and others don't, and Kam and Palmer (2008) list out variables associated with each attribute on page 7. We select four variables from each attribute, plus four demographic variables, for a total of 20 covariates that we believe will predict college attendance. These are listed in the true_covars vector below.   

```{r assigning variables}
true_covars <- c("student_GPA", "student_PubAff", "student_FamTalk", "student_Knowledge",
                 "student_GovtOpinion", "student_LifeWish", "student_Trust", "student_StrOpinion",
                 "student_Senate", "student_Govern", "student_Hobby", "student_YouthOrg",
                 "parent_HHInc", "parent_Knowledge", "parent_Employ", "parent_EducHH",
                 "student_Gen", "student_Race", "parent_Gen", "parent_Race")


# alternative outcomes
altoutcomevars <- c("student_vote", "student_meeting", "student_other", "student_button", 
                 "student_money", "student_communicate", "student_demonstrate", "student_community")

# covariates - exclude outcome vars, any vars from years after treatment
# additional exclude parent_GPHighSchoolPlacebo & parent_HHCollegePlacebo b/c sometimes missing
covars <- colnames(select(d, -interviewid, -college, -student_ppnscal, -all_of(altoutcomevars), 
                          -contains("198"), -contains("197"), 
                          -parent_GPHighSchoolPlacebo, -parent_HHCollegePlacebo))

# create dataframe without unnecessary covariates
d_base <- select(d, college, student_ppnscal, all_of(true_covars))
```

We estimate the propensity score by regressing assignment to treatment (college) on our 20 covariates. This gives the probability that a student will be in the treatment or control group. The plot below confirms that the propensity scores are higher for people in the treatment group (those that went to college).   

```{r propensity scores}
# estimate propensity scores
ps_est <- glm(as.formula(paste('college ~', paste(true_covars, collapse='+'))),
              family = binomial(), data = d)

# look at propensity score across treatment and control groups
d_base %>%
  mutate(ps = predict(ps_est)) %>%
  mutate(college = factor(college, labels = c("no college", "college"))) %>%
  ggplot() +
  geom_density(aes(x = ps, color = college)) +
  theme_bw() +
  labs(title = "Distribution of Propensity Score by Treatment")
```

#### Using MatchIt 

Rather than manually matching observations based on their propensity score, we use the MatchIt package to calculate the propensity scores, re-weight the observations based on their propensity scores, and then estimate the average treatment effect on the treated (ATT).

One problem is that there are fewer control units than treated units. We match with replacement so that each of our treated units can be matched to a control unit. A nearest neighbor match with k=1 results in about half our of control units not being matched. Increasing k to 3 means that more of our control units get matched to treatment units (because the threshold for a match is lower). Kam and Palmer also perform a 1 to 3 match, and set their caliper to 0.25 (which we do as well). The concern with increasing k is that units that are not that similar to one another become matched. However, setting the caliper in order to limit the distance between matches combats this. 

```{r matchit}
match_ps_att <- matchit(formula = as.formula(paste('college ~', paste(true_covars, collapse='+'))),
                        data = d, method = "nearest", distance = "glm", link = "logit", estimand = "ATT", replace = TRUE, ratio = 3, caliper = 0.25)
#summary(match_ps_att)

# statistics to report

# proportion of covariates that meet the standardized mean difference <= 0.1
share_bal <- bal.tab(match_ps_att, thresholds = 0.1)$Balance %>%
  transmute(balanced = 1*(M.Threshold == "Balanced, <0.1")) %>%
  unlist() %>%
  mean()
 
# mean percent improvement in the standardized mean difference
pct_improv <- summary(match_ps_att)$reduction[,1] %>%
  mean()
```

#### Assessing Balance 

The plot below shows the distribution of propensity scores by match type. There is a large enough region of common support that all of the treatment units get matched to a control unit. At the top of the propensity score distribution, a few matched units are getting matched many times to different treatment units (shown by the large circles). This is a case of extremity bias that Henderson and Chatfield point out as a problem with the Kam and Palmer paper.

The unmatched control units are the ones with the lowest propensity scores. These units will not be included in the model because, like never-takers, we can never observe their counterfactual of what would have happened if they did go to college. We will loose some statistical power from throwing out these units.

```{r plot ps distribution}
plot(match_ps_att, type = "jitter", interactive = FALSE)
```

We also look at the balance of our 20 covariates across the treatment and control groups for pre- and post-matching. The plot below shows the absolute standardized mean difference between the treatment and control groups before and after matching for each of our covariates. We see that matching has improved the overall balance, because fewer covariates have large differences between the means. Using a threshold of 0.1 to indicate balance, our matched model has 13 out of 21 balanced covariates (including the newly created distance variable as a covariate) - meaning that the proportion of balanced covariates is `r round(share_bal, 2)`. This compares to only 2 covariates that were balanced before matching. 

```{r plot SMD improvements}
plot(summary(match_ps_att))

#bal.tab(match_ps_att, threshold = 0.1)

# covariates that are balanced at the 0.1 threshold
bal_covars <- c("student_GPA", "student_FamTalk", "student_GovtOpinion", "student_LifeWish", "student_Senate", "student_Govern", "parent_Employ", "student_Gen", "student_Race", "parent_Race")
```

The table below shows the percent improvement in balance for each of the covariates. Despite a large increase in the *number* of balanced covariates, matching did not improve the mean difference between control and treatment *overall.* The mean percent improvement is `r round(pct_improv, 4)` - the negative percent indicates that it got worse. This is mainly due to the substantially worse balance for the student_PubAff variable.  

```{r pct improvement table}
summary(match_ps_att)$reduction
```

#### Estimating the ATT

```{r estimating att}
# estimate ATT 
match_ps_att_data <- match.data(match_ps_att) 

lm_ps_att <- lm(as.formula(paste('student_ppnscal ~ college +', paste(true_covars, collapse='+'))), 
                data = match_ps_att_data, weights = weights)

# store results
lm_ps_att_summ <- summary(lm_ps_att)

# ATT
ATT_ps <- lm_ps_att_summ$coefficients["college", "Estimate"]

# p-value for ATT
ATT_pv <- lm_ps_att_summ$coefficients["college", "Pr(>|t|)"]
```

After matching using propensity scores and k-nearest neighbors, we used the weighted observations estimate the average treatment on the treated (ATT). The ATT is `r ATT_ps` and is significant at the 1% threshold. This indicates that going to college is associated with participating in nearly one more political activity later in life (for those that go to college). If the selection on observables assumption holds - that conditioning on these 20 covariates assignment to treatment is as good as randomly assigned - then we can interpret this as a causal relationship.   

```{r model results}
lm_ps_att_summ
```


### 4.2: Simulation
To investigate how sensitive the ATT estimate is to changes in the covariate specification, we run the propensity score model from above 10,000 times. Each time we randomly sample a subset of the 110 potential covariates from Henderson and Chatfield's list. The function below: 

- randomly samples a subset of covariates
- estimates the propensity score model using MatchIt
- estimates the results ATT
- records the ATT, the share of covariates that are balanced, and the mean percentage improvement in balance 


```{r paul try}
#set simulation parameters
set.seed(1)
iters <- 10000

#generate random selection of covars
n <- sample(2:length(covars), iters, replace = TRUE)
covar_sample <- sapply(n, function(x) sample(covars, size = x))


#store results as a matrix (converting to data frame kept crashing my computer :/)
# first column is att
# second is p-value
# third is balance proportion
# fourth is percent improvement
# fifth is number of covariates 


saveRDS(results, "part4_ps_simulation_results_10000.rds")

# only run once 
if (RUNSIM == TRUE) {

  results <- t(mapply(function(x,y) 
      list(
        # grabs lm parameters using hero's code (i love it!! so brilliant)
        summary(lm(as.formula(paste('student_ppnscal ~ college +', paste(x, collapse='+'))), 
                    data = match.data(y), weights = weights))$coefficients["college", "Estimate"],
        
        
        summary(lm(as.formula(paste('student_ppnscal ~ college +', paste(x, collapse='+'))), 
                    data = match.data(y), weights = weights))$coefficients["college", "Pr(>|t|)"],
        
        
        # balance proportion   
      bal.tab(y, thresholds = 0.1)$Balance %>%
      transmute(balanced = 1*(M.Threshold == "Balanced, <0.1")) %>%
      unlist() %>%
      mean(),
      
      
        #percent improvement
      
      summary(y)$reduction[,1] %>% mean()
      
      ), 
      
      
      #data x referred to above
      
      covar_sample, 
      
      #data y
      lapply(covar_sample, function(z) suppressWarnings(matchit(formula = as.formula(paste('college ~', 
                                                                                           paste(z, collapse='+'))),
                           data = d, method = "nearest", distance = "glm", link = "logit", 
                           estimand = "ATT", replace = TRUE, ratio = 3, caliper = 0.25)))
  ))
  
  results <- cbind(results, n)
  colnames(results) <- c("att", "pvalue", "sharebal", "pctimprov", "ncovars")
  
  # save results
  saveRDS(results, "part4_ps_simulation_results_10000.rds")
}
if (RUNSIM == FALSE){

  results <- readRDS("part4_ps_simulation_results_10000.rds")
  
}

```

We run the above simulation 10,000 times and save a matrix with the returned results.  

The distribution of estimates for the ATT is shown below, with our "true" ATT shown in red. Other than for the models that only had 1, 2 or 3 covariates, the p-value for the ATT was always less than 0.01, indicating that the estimate is significant at the 1% level. The plot shows a slightly right skewed normal distribution of estimated ATTs, with our "true" estimate close to the mean. Our estimates for the ATT are clustered between K&P's estimate and H&C's estimates.  

```{r, echo = F}
#results %>%
  ggplot() +
  geom_histogram(aes(x = unlist(results[,1]))) +
  theme_bw() +
  ggtitle("Distribution of ATTs",
          subtitle = "'True' model shown in red") +
  geom_vline(xintercept = ATT_ps, color = "red") +
  xlab("ATT")
```

The plot below shows the share of covariates that meet the balance threshold of 0.1 against the estimate for the average treatment on the treated. There is not any clear relationship between the share of balanced covariates and the estimated ATT. There does appear to be some bunching around particular shares of covariates. The plot does not show convergence of ATT like in the K&P paper, but this might be due to our lower number of simulations.  Our "true" model sits pretty squarely in the middle of the distribution of the share of balanced covariates.  

```{r, echo = F}
  ggplot() +
  geom_point(aes(y = unlist(results[,1])), x = unlist(results[,3])) +
  theme_bw() +
  labs(title = "ATT against Proportion of Balanced Covariates",
       subtitle = "'True' model shown in red") +
  xlab("Share") +
  ylab("Average Treatment on the Treated") +
  geom_point(aes(y = ATT_ps, x = share_bal, color = "red")) +
  theme(legend.position="none")

```

The next plot shows the mean percentage increase in the balance of the covariates after matching (across all the covariates used in that simulation) against the estimate for the ATT. The larger the improvement in the balance of covariates relative to the unmatched units the higher the ATT. Our "true" model had a relatively low degree of mean improvement from matching, mainly driven by one covariate that became very unbalanced after matching. 

```{r, echo = F}
  ggplot() +
  geom_point(aes(x = unlist(results[,4]), y = unlist(results[,1]))) +
  theme_bw() +
  labs(title = "ATT against Mean Percent Improvement",
       subtitle = "'True' model shown in red") +
  xlab("Percent") +
  ylab("Average Treatment on the Treated") +
  geom_point(aes(x = pct_improv, y = ATT_ps, color = "red")) +
  theme(legend.position="none")
```

Finally, we show the number of covariates against the ATT. This plot shows convergence toward an ATT of around 0.8 as the number of covariates increases. Interestingly, this is around the same value ATT we got from our "true" model, suggesting that increasing the number of covariates to match on does not improve the model relative to correctly specifying the covariates to match on.    

```{r, echo = F}
  ggplot() +
  geom_point(aes(y = unlist(results[,1]), x = n)) +
  theme_bw() +
  labs(title = "ATT against Number of Covariates",
       subtitle = "'True' model shown in red") +
  xlab("Number") +
  ylab("Average Treatment on the Treated") +
  geom_point(aes(y = ATT_ps, x = 20, color = "red")) +
  theme(legend.position="none")
```

#### Ten Example Simulations 
We then select 10 models at random and show the covariate balance plots for each of the 10 models. Because some of the randomly selected models might use a large number of covariates that become difficult to show visually, select only five covariates to display for each model. The five that we select are the ones that appear most in the other 9 models, so that we can asses whether or not the balance improvement changes for covariate x when it is in a model with different other covaraites.  


```{r}
# set seed
set.seed(1)

# function for simulation 
ps_match_sim <- function(save_model = FALSE){ 
 
  # 1. randomly sample N covariates 
  n = sample(1:length(covars),1)
  covar_sample <- sample(covars, size = n)
  
  # 2. run match it 
  match_model <- matchit(formula = as.formula(paste('college ~', paste(covar_sample, collapse='+'))),
                         data = d, method = "nearest", distance = "glm", link = "logit", 
                         estimand = "ATT", replace = TRUE, ratio = 3)
  
  # 3. calculate ATT 
  match_data <- match.data(match_model) 
  
  lm_model <- lm(as.formula(paste('student_ppnscal ~ college +', paste(covar_sample, collapse='+'))), 
                  data = match_data, weights = weights)
  
  # 4. store results 
  results <- c()
  
  # ATT
  att <- summary(lm_model)$coefficients["college", "Estimate"]
  results[1] = att  
  
  # p-value for ATT
  att_pv <- summary(lm_model)$coefficients["college", "Pr(>|t|)"]
  results[2] = att_pv
  
  # number of covariates (stored in n above)
  results[3] = n
  
  # proportion of covariates that meet the standardized mean difference <= 0.1
  share_bal <- bal.tab(match_model, thresholds = 0.1)$Balance %>%
    transmute(balanced = 1*(M.Threshold == "Balanced, <0.1")) %>%
    unlist() %>%
    mean()
  results[4] = share_bal
   
  # mean percent improvement in the standardized mean difference
  pct_improv <- summary(match_model)$reduction[,1] %>%
    mean()
  results[5] = pct_improv
  
  results <- round(results, digits = 4)
  
  # if on, save all model attributes and list of covars
  if (save_model == TRUE){
    
    results = list(results,
                   match_model,
                   covar_sample)
  }
  
  return(results)
}
```


```{r, message=FALSE}
# run 10 times
sim10 <- replicate(10, ps_match_sim(save_model = T), simplify = FALSE)

# function to return df with covariate, pre- and post-absolute standardized mean differenace
balance_plot <- function(i = 1){
  
  # number of covars + 1 
  k <- length(sim10[[i]][[3]])+1

  p_df <- data.frame(vars = sim10[[i]][[3]],
                     dif_pre = unname(abs(summary(sim10[[i]][[2]])$sum.all[2:k,3])),
                     dif_post = unname(abs(summary(sim10[[i]][[2]])$sum.matched[2:k,3])),
                     sim = i,
                     ncov = k-1)
  
  # take a sample of min(10,n) covars for plot
  #s_size <- min(10, nrow(p_df))
  #p_df <- sample_n(p_df, s_size)
  
  return(p_df)
}

# run for all 10 models
df10 <- lapply(1:10, balance_plot) %>% bind_rows()

# limit to 5 covariates per model based on overlap with other models 
df10 <- df10 %>%
  group_by(vars) %>%
  mutate(freq = n()) %>%
  group_by(sim) %>%
  arrange(-freq, .by_gruop = TRUE) %>%
  slice(1:5)


# show balance plots for the 10 models
ggplot(df10) +
  geom_point(aes(x = dif_pre, y = vars, color = "pre")) +
  geom_point(aes(x = dif_post, y = vars, color = "post")) +
  geom_vline(xintercept = 0.1) +
  xlab("Abs. Std. Mean Difference") +
  ylab("") +
  facet_wrap("sim", nrow = 2) +
  theme_bw() +
  labs(title = "Comparison of Balance for 10 Simulations",
       caption = "Not all plots have all the covariates b/c they are randomly selected in each simulation.")
```


### 4.3: Questions 

**1. How many simulations resulted in models with a higher proportion of balanced covariates? Do you have any concerns about this?**
Our "true" model sat squarely in the middle of the simulations in terms of the share of balanced covariates. Given that the ATT did not change much with the share of balanced covariates, we are not concerned that our true model was more balanced than only half of the simulation models. The range of the share of balanced covariates was also not that large - the least balanced were around 56% and the most around 68%. 

**2. Analyze the distribution of the ATTs. Do you have any concerns about this distribution?**
The distribution of ATTs is roughly normal, with a slight right skew. The lower bound of the distribution is around 0.5, meaning none of our simulations produced an ATT as low as the one in K&P's paper. Looking at the distribution of the ATT against the number of covariates matched on shows some convergence toward an ATT of around 0.8, which is also what we estimated in our "true" model. The more covariates matched on, the less variance there is in the ATT estimate, which is good for precision. However, this does not preclude the possibility that all the estimates are biased. 

**3. Do your 10 randomly chosen covariate balance plots produce similar numbers on the same covariates? Is it a concern if they do not?**
Most of the covariate balance plots show similar numbers on the same covariates *post* matching. In general, covariates that were imbalanced before matching and balanced after matching display this trend regardless of the model they're in. This is encouraging because it implies that the treatment and control groups have similar characteristics across the different models post matching. If they did not have similar balance plots across models it would suggest that observations were being matched differently depending on the combination of covariates that are matched on. If an observation can get matched differently across models then it is like saying there are multiple counterfactuals for one observation and we would probably have an imprecisely estimated ATT.     

## 5: Questions 5

### 5.1: Simulate Alternative Model

As an alternative strategy, we use genetic matching on the same set of 20 covariates that we believe are most predictive of college attendance. Genetic matching uses a genetic search algorithm to iteratively test various weights for the matching covariates. We again use the same matching parameters, including a logit model for determining propensity scores, matching with replacement, using a matching ratio of 3, and a caliper of 0.25.

```{r}
system.time(
  match_gen_att <- matchit(formula = as.formula(paste('college ~', paste(true_covars, collapse='+'))),
                        data = d_base, method = "genetic", distance = "glm", link = "logit", 
                        estimand = "ATT", replace = TRUE, ratio = 3, 
                        caliper = 0.25
                        )
)


#summary(match_gen_att)

# statistics to report

# proportion of covariates that meet the standardized mean difference <= 0.1
gen_share_bal <- bal.tab(match_gen_att, thresholds = 0.1)$Balance %>%
  transmute(balanced = 1*(M.Threshold == "Balanced, <0.1")) %>%
  unlist() %>%
  mean()
 
# mean percent improvement in the standardized mean difference
gen_pct_improv <- summary(match_gen_att)$reduction[,1] %>%
  mean()
```

This model performs somewhat better than the baseline nearest-neighbor matching model with the same 20 covariates, with 76% of covariates balanced (compared with 62%), and a slightly more positive percent improvement in the standardized mean difference (-69% compared with -85%). Iteratively deriving optimal weights for each covariate via genetic matching produces a more balanced outcome.

The computational burden of genetic matching precludes 10,000 simulations, as each individual match can take several minutes to run. We produce 500 simulations using the genetic matching algorithm, which we then compare with the 10,000 simulations previously produced using the nearest-neighbor approach.

```{r}
#set simulation parameters
set.seed(1)
iters <- 500

#generate random selection of covars
#n <- sample(1:length(covars), iters, replace = TRUE)
n <- sample(2:30, iters, replace = TRUE)
covar_sample <- sapply(n, function(x) sample(covars, size = x))

#store results as a matrix (converting to data frame kept crashing my computer :/)
# first column is att
# second is p-value
# third is balance proportion
# fourth is percent improvement
# fifth is number of covariates 

# only run once 
if (RUNSIM == TRUE) {

  for(loop in 1:(iters/100)){
    cat(loop)
    gen_results_temp <- t(mapply(function(x,y) 
      list(
        # grabs lm parameters using hero's code (i love it!! so brilliant)
        summary(lm(as.formula(paste('student_ppnscal ~ college +', paste(x, collapse='+'))), 
                    data = match.data(y), weights = weights))$coefficients["college", "Estimate"],
        
        summary(lm(as.formula(paste('student_ppnscal ~ college +', paste(x, collapse='+'))), 
                    data = match.data(y), weights = weights))$coefficients["college", "Pr(>|t|)"],
        
        # balance proportion   
      bal.tab(y, thresholds = 0.1)$Balance %>%
      transmute(balanced = 1*(M.Threshold == "Balanced, <0.1")) %>%
      unlist() %>%
      mean(),
      
        #percent improvement
      summary(y)$reduction[,1] %>% mean(),
      length(x)
      ), 
      
      #data x referred to above
      covar_sample[(loop*100-99):(loop*100)], 
      
      #data y
      mclapply(covar_sample[(loop*100-99):(loop*100)], 
               function(z) suppressWarnings(matchit(formula = as.formula(paste('college ~', paste(z, collapse='+'))),
                           data = d %>% select_at(c("college", "student_ppnscal", z)), 
                           method = "genetic", distance = "glm", link = "logit", 
                           estimand = "ATT", replace = TRUE, ratio = 3, caliper = 0.25)),
             mc.cores = getOption("mc.cores", detectCores()-1))
      ))
    if(exists("gen_results")){
      gen_results <<- rbind(gen_results, gen_results_temp)
    } else {
      gen_results <<- gen_results_temp
    }
  }

  colnames(gen_results) <- c("att", "pvalue", "sharebal", "pctimprov", "ncovars")
  
  # save results
  saveRDS(gen_results, "part5_ps_simulation_results_500.rds")
}
if (RUNSIM == FALSE){

  gen_results <- readRDS("part5_ps_simulation_results_500.rds")
  
}
```

Genetic matching consistently produces models in which a greater share of the covariates are balanced (__%), relative to the nearest-neighbor matching where the median share of balanced covariates was 66.67%.

```{r, echo = FALSE}
ggplot() + 
  geom_density(data = as.data.frame(results) %>%
                 mutate(`Share Balanced` = as.numeric(sharebal)), 
               aes(`Share Balanced`), 
               fill = "lightblue", alpha = 0.5) +
  geom_density(data = as.data.frame(gen_results) %>%
                 mutate(`Share Balanced` = as.numeric(sharebal)), 
               aes(`Share Balanced`), 
               fill = "pink", alpha = 0.5)
```


Plotting the percent improvement in the simulated models based on genetic matching similarly reveals that genetic matching consistently performs better than nearest-neighbor matching.

```{r, echo = FALSE}
ggplot() + 
  geom_density(data = as.data.frame(results) %>%
                 mutate(`Percent Improvement` = as.numeric(pctimprov)), 
               aes(`Percent Improvement`), 
               fill = "lightblue", alpha = 0.5) +
  geom_density(data = as.data.frame(gen_results) %>%
                 mutate(`Percent Improvement` = as.numeric(pctimprov)), 
               aes(`Percent Improvement`), 
               fill = "pink", alpha = 0.5)
```

